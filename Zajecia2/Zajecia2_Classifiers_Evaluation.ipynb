{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Class 2 - Classifiers evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import needed modules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.style.use('seaborn-white') #changing plot style\n",
    "plt.rcParams['figure.dpi']=90.0 #size of figures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KernelDensity\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Splitting dataset into train and validation subsets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data loading & pre-processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/german/german.data-numeric\"\n",
    "DATA_SET = pd.read_fwf(url, header = None)\n",
    "DATA_SET.rename(columns = {24: \"target\"}, inplace=True)\n",
    "DATA_SET['target'] = DATA_SET['target'] - 1 #recoding target variable\n",
    "DATA_SET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "X = DATA_SET.drop(['target'], axis=1)\n",
    "y = DATA_SET['target']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape, X_test.shape)\n",
    "print(y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is random sampling the best approach? What if one class has many more records than the other?\n",
    "Imbalanced data may lead to poor model which may have good overall performance metrics e.g. accuracy.\n",
    "\n",
    "There are several approaches to tackle the issue:\n",
    "- undersampling, \n",
    "- oversampling, \n",
    "- cost-based performance,\n",
    "- algorithmic approches e.g.SMOTE\n",
    "\n",
    "Quick reading with code samples:\n",
    "[Article on imbalanced data in Python](https://towardsdatascience.com/methods-for-dealing-with-imbalanced-data-5b761be45a18)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Cost-based approach in model assessment**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoff_analysis(y_test: pd.Series, y_test_hat: pd.Series, cost_matrix: np.array = np.array([[0,0],[0,0]]) ) -> list:\n",
    "    \"\"\"\n",
    "    Calculate accuracy vector for cutoff thresholds between 0 and 1 for given true labels `y_test` \n",
    "    and predicted labels `y_test_hat`. If `cost_matrix` is specified, calculates cost vector instead.\n",
    "    \"\"\"\n",
    "    cutoff_range = np.arange(0, 1.0, 0.01)\n",
    "    vec = []\n",
    "    for cutoff in cutoff_range:\n",
    "        y_test_hat_bin = np.where(y_test_hat >= cutoff, 1, 0)\n",
    "        conf_mat = confusion_matrix(y_test, y_test_hat_bin)\n",
    "        #no cost matrix, calculate accuracy\n",
    "        if np.sum(cost_matrix) == 0:\n",
    "            vec.append(np.sum(np.diag(conf_mat)) / np.sum(conf_mat))\n",
    "        else:\n",
    "            conf_const_mat = np.multiply(conf_mat, cost_matrix)\n",
    "            vec.append(conf_const_mat.sum() / len(y_test))\n",
    "    return vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Building logistic regression model**\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/8/88/Logistic-curve.svg/320px-Logistic-curve.svg.png\" align=\"left\">\n",
    "\n",
    "https://en.wikipedia.org/wiki/Logistic_regression\n",
    "\n",
    "https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LogisticRegression(max_iter=10000)\n",
    "LR_L1 = model.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LR_L1.coef_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Measuring model performance (cost-based) for different cutoof thresholds and with or without validation dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#On validation data\n",
    "score_val = LR_L1.predict_proba(X_test)[:,1]\n",
    "#On training data\n",
    "score_train = LR_L1.predict_proba(X_train)[:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "costmat = np.array([[0,1],[5,0]])\n",
    "cost_val = cutoff_analysis(y_test, score_val, cost_matrix = costmat)\n",
    "cost_train = cutoff_analysis(y_train, score_train, cost_matrix = costmat)\n",
    "costmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Cutoff point\")\n",
    "plt.ylabel(\"Cost per client\")\n",
    "plt.title(\"Cost vs. cut-off\")\n",
    "\n",
    "plt.plot(np.arange(0, 1.0, 0.01), cost_val)\n",
    "plt.plot(np.arange(0, 1.0, 0.01), cost_train, color = \"red\")\n",
    "plt.plot([0, 1], [min(cost_val), min(cost_val)], color = 'gray', label = \"Min Cost Val = \" + \\\n",
    "         str(round(min(cost_val),3)) + \" for k = \" + str(round(np.arange(0, 1.0, 0.01)[cost_val.index(min(cost_val))],2)))\n",
    "plt.plot([0, 1], [min(cost_train), min(cost_train)], color = 'gray', linestyle = \":\", label = \"Min Cost Train = \" + \\\n",
    "         str(round(min(cost_train),3)) + \" for k = \" + str(np.arange(0, 1.0, 0.01)[cost_train.index(min(cost_train))]))\n",
    "\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like we got lower cost for predictions on training set, but model may **overfit**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Assessing model performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = \"http://archive.ics.uci.edu/ml/machine-learning-databases/statlog/australian/australian.dat\"\n",
    "dataset = pd.read_csv(url, sep = \" \", header = None)\n",
    "dataset.columns = [\"V\" + str(i) for i in range(0,15)]\n",
    "dataset.rename(columns = {\"V14\": \"class\"}, inplace = True)\n",
    "\n",
    "dataset['V3'] = np.where(dataset['V3'] == 1, 0, 1)\n",
    "dataset['V11'] = np.where(dataset['V11'] == 1, 0, 1)\n",
    "dataset['V13'] = np.log(dataset['V13'])\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Data split using sklearn**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_fraction = 0.8\n",
    "X = dataset.iloc[:, 0:14]\n",
    "y = dataset.iloc[:, 14]\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 1-training_fraction, random_state = 42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Logistic Regression estimation with L1 regularization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model_log_reg = LogisticRegression(random_state = 42, solver = 'liblinear', penalty = 'l1')\n",
    "model_log_reg_fit = model_log_reg.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**L1 regularization allows for feature selection**\n",
    "\n",
    "https://en.wikipedia.org/wiki/Lasso_(statistics)\n",
    "\n",
    "Lasso = L1 on linear regression\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/6f7b5020a85afe0ca7e4ac1bcda7d193bc812617)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_log_reg_fit.coef_\n",
    "#First coefficient is 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = model_log_reg.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Confusion matrix**\n",
    "\n",
    "‚ùó Remember class indicator (0, 1,...) and actual or predicted values may be switched in confusion matrix\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/712/1*Z54JgbS4DUwWSknhDCvNTQ.png\" width=400>\n",
    "\n",
    "<img src=\"https://miro.medium.com/max/1780/1*LQ1YMKBlbDhH9K6Ujz8QTw.jpeg\"  width=400>\n",
    "\n",
    "**Performance measures derived from confusion matrix:**\n",
    "\n",
    "- Accuracy - percentage of correct predictions\n",
    "\n",
    "`ACC = (TP + TN)/(TP + FP + TN + FN)`\n",
    "\n",
    "- Precision - percentage of positive predictions which were actually correct\n",
    "\n",
    "`PREC = TP / (TP + FP)`\n",
    "\n",
    "-  Recall - what percentage of actual positives were predicted correctly\n",
    " (Recall = Sensitivity = Hit rate = True Positive Rate (TPR))\n",
    " \n",
    "`REC = TP / (TP + FN)`\n",
    "\n",
    "- Specificity - what percentage of actual negatives were predicted correctly (Specificity = True Negative Rate)\n",
    "\n",
    "`TNR = TN / (TN + FP)`\n",
    "\n",
    "- F1 Score - traditional F-measure or balanced F-score (F1 score) is the harmonic mean of precision and recall\n",
    "\n",
    "![](https://wikimedia.org/api/rest_v1/media/math/render/svg/1bf179c30b00db201ce1895d88fe2915d58e6bfd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "confm = confusion_matrix(y_test, y_test_hat)\n",
    "confm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is what in that confusion matrix?\n",
    "confusion_matrix?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ACC= (confm[0,0]+confm[1,1])/(confm[0,0]+confm[1,1]+confm[0,1]+confm[1,0])\n",
    "PREC = (confm[1,1])/(confm[1,1]+confm[0,1])\n",
    "REC = (confm[1,1])/(confm[1,1]+confm[1,0])\n",
    "TNR = (confm[0,0])/(confm[0,0]+confm[0,1])\n",
    "F1 = 2*PREC*REC/(PREC+REC)\n",
    "print(\"ACC \",ACC,\"\\nPREC \",PREC,\"\\nREC \",REC,\"\\nTNR \",TNR,\"\\nF1 \",F1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Sklearn built-in report\n",
    "print(classification_report(y_test,y_test_hat))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Finding optimal cut-off based on ACC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_test_hat = model_log_reg.predict_proba(X_test)[:,1]\n",
    "y_train_hat = model_log_reg.predict_proba(X_train)[:,1]\n",
    "acc_k = cutoff_analysis(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.xlabel(\"Cutoff point\")\n",
    "plt.ylabel(\"Accuracy\")\n",
    "plt.title(\"Accuracy vs. cut-off\")\n",
    "\n",
    "plt.plot(np.arange(0, 1.0, 0.01), acc_k)\n",
    "plt.plot([0, 1], [max(acc_k), max(acc_k)], color = 'black', linestyle = \":\", label = \"Max ACC = \" + \\\n",
    "         str(round(max(acc_k),3)) + \" for k = \" + str(np.arange(0, 1.0, 0.01)[acc_k.index(max(acc_k))]))\n",
    "plt.legend();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visual analysis of a model\n",
    "\n",
    "**Gain&Lift&ROC curves**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gain_lift_roc_plot(y_test: pd.Series, y_test_hat: pd.Series, return_chart: str, n: int):\n",
    "    \"\"\"\n",
    "    Produces Gain, Lift or ROC curve based on provided `y_test` true labels and `y_test_hat` prediction labels. \n",
    "    Wizard and Random models are added for comparison.\n",
    "    \"\"\"\n",
    "    cutoff_range = np.arange(0, 1.0, 0.01)\n",
    "    rpp = [0]*len(cutoff_range)\n",
    "    tpr = rpp.copy()\n",
    "    lift = rpp.copy()\n",
    "    fpr = rpp.copy()\n",
    "    prc = rpp.copy()\n",
    "    x0 = np.mean(y_test)\n",
    "    plt.subplot(2, 2, n)\n",
    "    for i, cutoff in enumerate(cutoff_range):\n",
    "        y_test_hat_bin = np.where(y_test_hat >= cutoff, 1, 0)\n",
    "        conf_mat = confusion_matrix(y_test, y_test_hat_bin)\n",
    "        rpp[i] = np.sum(conf_mat[:,1]) / np.sum(conf_mat)\n",
    "        fpr[i] = conf_mat[0,1] / np.sum(conf_mat[0,:])\n",
    "        tpr[i] = conf_mat[1,1] / np.sum(conf_mat[1,:])\n",
    "        prc[i] = conf_mat[1,1] / np.sum(conf_mat[:,1])\n",
    "        lift[i] = tpr[i] / rpp[i]\n",
    "    for i, cutoff in enumerate(cutoff_range):\n",
    "        y_test_hat_bin = np.where(y_test_hat >= cutoff, 1, 0)\n",
    "        conf_mat = confusion_matrix(y_test, y_test_hat_bin)\n",
    "        rpp[i] = np.sum(conf_mat[:,1]) / np.sum(conf_mat)\n",
    "        fpr[i] = conf_mat[0,1] / np.sum(conf_mat[0,:])\n",
    "        tpr[i] = conf_mat[1,1] / np.sum(conf_mat[1,:])\n",
    "        prc[i] = conf_mat[1,1] / np.sum(conf_mat[:,1])\n",
    "        lift[i] = tpr[i] / rpp[i]\n",
    "    if return_chart == \"gain\":\n",
    "        plt.xlabel(\"Rate of Positive Predictions\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.plot(rpp, tpr, color=\"orange\", label = \"Model\")\n",
    "        plt.plot([0,1],[0,1], color='grey', linestyle=\"--\", label = \"Random\") # random\n",
    "        plt.plot([0,x0],[0,1], color='navy', linestyle=':', label = \"Wizard\") # wizard\n",
    "        plt.plot([x0,1],[1,1], color='navy', linestyle=':') # wizard\n",
    "        plt.legend(loc = \"lower right\")\n",
    "        plt.title(\"Gain chart\")\n",
    "    elif return_chart == \"lift\":\n",
    "        plt.xlabel(\"Rate of Positive Predictions\")\n",
    "        plt.ylabel(\"Lift\")\n",
    "        plt.plot(rpp, lift, color=\"orange\", label = \"Model\")\n",
    "        plt.plot([0,1],[1,1], color='grey', linestyle=\"--\", label = \"Random\") # random\n",
    "        plt.plot([0,x0],[1/x0,1/x0], color='navy', linestyle=':', label = \"Wizard\") # wizard\n",
    "        plt.plot([x0,1],[1/x0,1], color='navy', linestyle=':') # wizard\n",
    "        plt.legend(loc = \"upper right\")\n",
    "        plt.title(\"Lift chart\")\n",
    "    elif return_chart == \"roc\":\n",
    "        plt.xlabel(\"False Positive Rate\")\n",
    "        plt.ylabel(\"True Positive Rate\")\n",
    "        plt.plot(fpr, tpr, color=\"orange\", label = \"Model\")\n",
    "        plt.plot([0,1],[0,1], color='grey', linestyle=\"--\", label = \"Random\") # random\n",
    "        plt.plot([0,0],[0,1], color='navy', linestyle=':', label = \"Wizard\") # wizard\n",
    "        plt.plot([0,1],[1,1], color='navy', linestyle=':') # wizard\n",
    "        plt.legend(loc = \"lower right\")\n",
    "        plt.title(\"ROC chart\")\n",
    "    plt.tight_layout(pad=0.4, w_pad=1.0, h_pad=1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "1/np.mean(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plt.rcParams['figure.figsize'] = [10, 8]\n",
    "gain_lift_roc_plot(y_test, y_test_hat,\"gain\",1)\n",
    "gain_lift_roc_plot(y_test, y_test_hat,\"lift\",2)\n",
    "gain_lift_roc_plot(y_test, y_test_hat, \"roc\",3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**ROC curve + AUC**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fprv, tprv, _ = roc_curve(y_test, y_test_hat)\n",
    "fprt, tprt, _ = roc_curve(y_train, y_train_hat)\n",
    "auc_rocv = auc(fprv, tprv)\n",
    "auc_roct = auc(fprt, tprt)\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title(\"ROC&AUC using sklearn\")\n",
    "\n",
    "plt.plot([0, 1], [0, 1], color='grey', linestyle='--', label = \"Random, AUC = 0.5\")\n",
    "plt.plot([0, 0], [0, 1], color='navy', linestyle=':', label = \"Wizard, AUC = 1.0\")\n",
    "plt.plot([0, 1], [1, 1], color='navy', linestyle=':')\n",
    "\n",
    "plt.plot(fprt, tprt, color = 'orange', label='Model - train, AUC = %0.2f' % auc_roct)\n",
    "plt.plot(fprv, tprv, color = 'red', label='Model - val, AUC = %0.2f' % auc_rocv)\n",
    "plt.legend(loc=\"lower right\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Score-density plot**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_density_plot(y_test: pd.Series, y_test_hat: pd.Series):\n",
    "    basis_x = np.linspace(-0.2,1.2,1000)\n",
    "    y_test_hat_0 = np.array(y_test_hat[y_test == 0])\n",
    "    y_test_hat_1 = np.array(y_test_hat[y_test == 1])\n",
    "    wizard_0 = np.array(y_test[y_test == 0])\n",
    "    wizard_1 = np.array(y_test[y_test == 1])\n",
    "    avg = np.mean(y_test)\n",
    "    ran = [1]\n",
    "    for i in range(0,len(y_test)-1):\n",
    "        ran = ran + [int(np.mean(ran) < avg)]\n",
    "    ran = np.array(ran)\n",
    "    random_0 = np.linspace(0,1.0,len(y_test))[ran==0]\n",
    "    random_1 = np.linspace(0,1.0,len(y_test))[ran==1]\n",
    "    res = []\n",
    "    for m in [y_test_hat_0, y_test_hat_1, wizard_0, wizard_1, random_0, random_1]:\n",
    "        kde = KernelDensity(kernel='epanechnikov', bandwidth=0.20)\n",
    "        kde.fit(m[:, None])\n",
    "        prob = np.exp(kde.score_samples(basis_x[:, None]))\n",
    "        res.append(prob)\n",
    "        \n",
    "    plt.figure()\n",
    "    plt.plot(basis_x, res[0], alpha=0.5, color = \"orange\", lw=2, label = \"Model\")\n",
    "    plt.plot(basis_x, res[1], alpha=0.5, color = \"orange\", linestyle=\":\", lw=2)\n",
    "    plt.plot(basis_x, res[2], alpha=0.5, color = \"navy\", label = \"Wizard\")\n",
    "    plt.plot(basis_x, res[3], alpha=0.5, color = \"navy\", linestyle=\":\")\n",
    "    plt.plot(basis_x, res[4], alpha=0.5, color = \"grey\", label = \"Random\")\n",
    "    plt.plot(basis_x, res[5], alpha=0.5, color = \"grey\", linestyle=\":\")\n",
    "    plt.title(\"Score density plot\")\n",
    "    plt.xlabel(\"Score\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend(loc = \"upper center\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "a = score_density_plot(y_test, y_test_hat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercises"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Iris dataset from https://raw.githubusercontent.com/uiuc-cse/data-fa14/gh-pages/data/iris.csv to 'iris' DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code `species` column to have value 1 if iris is from _versicolor_ species and 0 otherwise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split dataset to train and validation subsets using `train_test_split` function. Training set should have **75%** of all observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build logistic regression (with `LogisticRegression` from `sklearn`) using **Elastic-net** regularization with 0.35 L1 ratio (only one solver supports that, check [here](https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression))\n",
    "\n",
    "You can read more about **Elastic-net** [here](https://en.wikipedia.org/wiki/Elastic_net_regularization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make classification report with `classification_report`. What is accuracy of the model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Why accuracy is so low? If you want to know check [here](https://jakevdp.github.io/PythonDataScienceHandbook/05.02-introducing-scikit-learn.html#Unsupervised-learning-example:-Iris-dimensionality) below `In[19]`. Plot shows how target classes are distributed in 2D space (which was possible due to dimensionality reduction technique PCA - note that we have 4 predictors (sepal_length/width,petal_length/width) not 2). Remember we merged setosa and virginica species - knowing that look were versicolor is on the plot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write function \n",
    "\n",
    "`plot_acc_train_vs_val(y_train, y_test, y_train_hat, y_test_hat)` \n",
    "\n",
    "that takes following arguments:\n",
    "\n",
    "- y_train - array of class labels (0 or 1) for training data\n",
    "- y_test - array of class labels (0 or 1) for validation data \n",
    "- y_train_hat - array of probabilities (0 to 1) for class 1 for training data\n",
    "- y_test_hat - array of probabilities (0 to 1) for class 1 for validation data\n",
    "\n",
    "and produce plot like in **Finding optimal cut-off based on ACC** subsection but for both prediction on training and validation data. \n",
    "\n",
    "While creating function you _can_ use code as below:\n",
    "\n",
    "```python\n",
    "    acc_t = cutoff_analysis(y_train, y_train_hat)\n",
    "    acc_v = ...\n",
    "    plt.figure()\n",
    "    plt.xlabel(\"Cutoff point\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Accuracy vs. cut-off\")\n",
    "\n",
    "    plt.plot(np.arange(0, 1.0, 0.01), acc_t)\n",
    "    plt.plot(... , linestyle = \":\")\n",
    "    plt.plot([0, 1], [max(acc_t), max(acc_t)], color = 'gray', label = \"Max ACC train= \" + str(round(max(acc_t),3)) + \n",
    "             \" for k = \" + str(np.arange(0, 1.0, 0.01)[acc_t.index(max(acc_t))]))\n",
    "    plt.plot(.....................................)\n",
    "    plt.legend();\n",
    "```\n",
    "\n",
    "Then test your new function using (of course after filling placeholders):\n",
    "\n",
    "```python\n",
    "y_train_hat = model. ...\n",
    "y_test_hat = model. ...\n",
    "plot_acc_train_vs_val(y_train, y_test, y_train_hat, y_test_hat)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the plot you may see that accuracy for train and validation sets prediction id quite similar. Shouldn't the accuracy be better on the training set prediction? In this case not necessarily becuase model is **underfitted** - in other words it's biased and may perform better on validation set."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
